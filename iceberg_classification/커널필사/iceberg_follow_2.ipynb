{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statoil/C-CORE Iceberg Classifier Challenge \n",
    "\n",
    "#### Kaggle kernel 필사하기 - 2\n",
    "Link: <https://www.kaggle.com/cbryant/keras-cnn-statoil-iceberg-lb-0-1995-now-0-1516>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import cv2 # Used to manipulated the images \n",
    "np.random.seed(0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, Activation\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('train.json')\n",
    "#test = pd.read_json('test.json')\n",
    "target = train['is_iceberg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 5)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image data Preprocessing\n",
    "본 커널에서는 이미지 데이터를 전처리를 진행하고 모델링을 진행했다. 이미지 데이터 전처리 방법은 다음과 같다.\n",
    " 1. `band_1`, `band_2` 분리하여 독립적인 이미지 데이터를 만들고 `band_1`과 `band_2`의 데이터 값을 합친 이미지인 `band_3`도 생성한다.\n",
    " 2. 이미지 데이터 값의 범위를 scaling하여 모델이 좀 더 robust 하도록 만든다. 이 커널에서는 Standardizaton(표준화)를 진행했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image data scaling 함수 정의\n",
    "def get_scaled_img(data):\n",
    "    imgs = []\n",
    "    for i, row in data.iterrows():\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = band_1 + band_2\n",
    "        \n",
    "        # scaling\n",
    "        sc1 = (band_1 - band_1.mean()) / (band_1.max() - band_1.min())\n",
    "        sc2 = (band_2 - band_2.mean()) / (band_2.max() - band_2.min())\n",
    "        sc3 = (band_3 - band_3.mean()) / (band_3.max() - band_3.min())\n",
    "        \n",
    "        imgs.append(np.dstack((sc1, sc2, sc3)))\n",
    "        \n",
    "    return np.array(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = get_scaled_img(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train['is_iceberg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image data augmentation\n",
    "CNN 모델링을 할 때, 주어진 이미지로만 학습을 하면 데이터가 부족한 경우도 있고, 이미지의 각도, 색, 위치가 다양하기 때문에 일관된 학습을 하기 힘들다는 단점이 있다. 이 문제를 보완하기 위해 본 커널에서는 **image data augmentation**을 진행했다.\n",
    "\n",
    "Data augmentation 기법은 매우 다양하다. 대표적으로 이미지를 뒤집는 flip, 회전시키는 rotation, 색을 모두 회색으로 바꾸는 gray scale 등등이 있다. 본 커널에서는 수직, 수평으로 뒤집은 이미지를 추가로 더해주어 데이터를 증강했다.\n",
    "\n",
    " + 사용 패키지: `opencv` \n",
    " \n",
    " \n",
    "cf) 본 커널에서는 data augmentation을 위해 opencv 패키지를 사용했지만 tf.keras를 이용해 augmentation을 할 수 있다. 이 부분에 대한 코드도 따로 찾아보고 공부해볼 예정이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image data augmentation - vertical/horizontally flipped images\n",
    "def get_more_imgs(imgs):\n",
    "    more_imgs = []\n",
    "    vert_flip_imgs = []\n",
    "    horz_flip_imgs = []\n",
    "    \n",
    "    for i in range(imgs.shape[0]):\n",
    "        img1 = imgs[i, :, :, 0]\n",
    "        img2 = imgs[i, :, :, 1]\n",
    "        img3 = imgs[i, :, :, 2]\n",
    "        \n",
    "        # cv2.flip(a, 1): vertical line 기준으로 뒤집음, cv2.flip(a, 0): horizontal line 기준으로 뒤집음\n",
    "        img1_v = cv2.flip(img1, 1)\n",
    "        img1_h = cv2.flip(img1, 0)\n",
    "        img2_v = cv2.flip(img2, 1)\n",
    "        img2_h = cv2.flip(img2, 0)\n",
    "        img3_v = cv2.flip(img3, 1)\n",
    "        img3_h = cv2.flip(img3, 0)\n",
    "        \n",
    "        vert_flip_imgs.append(np.dstack((img1_v, img2_v, img3_v)))\n",
    "        horz_flip_imgs.append(np.dstack((img1_h, img2_h, img3_h)))\n",
    "    v = np.array(vert_flip_imgs)\n",
    "    h = np.array(horz_flip_imgs)\n",
    "    \n",
    "    more_imgs = np.concatenate((imgs, v, h))\n",
    "    \n",
    "    return more_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4812, 75, 75, 3)\n"
     ]
    }
   ],
   "source": [
    "X_new_train = get_more_imgs(X_train)\n",
    "print(X_new_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4812,)\n"
     ]
    }
   ],
   "source": [
    "y_new_train = np.concatenate((y_train, y_train, y_train))\n",
    "print(y_new_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model\n",
    "모델링 과정에 대한 공부 내용은 앞선 [필사](https://github.com/hyewonleess/Kaggle/blob/master/iceberg_classification/%EC%BB%A4%EB%84%90%ED%95%84%EC%82%AC/iceberg_follow_1.ipynb) 에서 자세히 다뤘으므로 넘어간다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    #Build keras model\n",
    "    \n",
    "    model=Sequential()\n",
    "    \n",
    "    # CNN 1\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3),activation='relu', input_shape=(75, 75, 3)))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # CNN 2\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu' ))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # CNN 3\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    #CNN 4\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # You must flatten the data for the dense layers\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #Dense 1\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    #Dense 2\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Output \n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    optimizer = Adam(lr=0.001, decay=0.0)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks(콜백)\n",
    "이번 커널에서도 callback이 등장했다. EarlyStopping과 ModelCheckpoint에 대해서는 이전 커널 필사에서 다뤘는데, 이번 커널에서는 `ReduceOnPlateu`가 새롭게 등장했다. \n",
    "\n",
    "#### What is ReduceOnPlateu?\n",
    "`ReduceOnPlateu`는 딥러닝 모델이 학습을 할 때 손실함수의 local minimum에 빠져버려 모델의 개선이 없을 경우, 학습률(learning rate)를 조절해 local minimum에서 빠져나오도록 하는 함수이다. 즉, 모델 성능이 개선이 더 이상 되지 않는 경우 학습률을 조정하여 모델이 조금 더 성능을 높일 수 있도록 도와주는 함수인 것이다.\n",
    "\n",
    " + monitor: ReduceOnPlatue의 기준이 되는 값  ex) val_loss(검증데이터셋의 loss)\n",
    " + factor: learning rate를 얼마나 감소시킬 것인지를 지정   ex) 현재 학습률이 0.1이고 factor가 0.5이면 새로운 학습률은 0.1 * 0.5 = 0.05\n",
    " + patience: 성능이 개선되지 않는 epoch를 몇 번 허용할 것인지를 지정\n",
    " \n",
    " \n",
    " <br>\n",
    " \n",
    " 추가적으로, 콜백은 여러개를 선언을 해 놓고 `model.fit`을 할 때 리스트로 묶어주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callbacks\n",
    "earlystopping = EarlyStopping(monitor = 'val_loss', patience = 10, verbose = 1, mode='min')\n",
    "mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor = 'val_loss', mode = 'min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 10, verbose = 1, mode = 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 73, 73, 64)        1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 5, 5, 64)          73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 560,193\n",
      "Trainable params: 560,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = getModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "106/106 [==============================] - 54s 495ms/step - loss: 0.6521 - accuracy: 0.5806 - val_loss: 0.4104 - val_accuracy: 0.8193\n",
      "Epoch 2/50\n",
      "106/106 [==============================] - 54s 509ms/step - loss: 0.4379 - accuracy: 0.7932 - val_loss: 0.3521 - val_accuracy: 0.8428\n",
      "Epoch 3/50\n",
      "106/106 [==============================] - 64s 606ms/step - loss: 0.3340 - accuracy: 0.8443 - val_loss: 0.3542 - val_accuracy: 0.8435\n",
      "Epoch 4/50\n",
      "106/106 [==============================] - 53s 496ms/step - loss: 0.3248 - accuracy: 0.8638 - val_loss: 0.3377 - val_accuracy: 0.8442\n",
      "Epoch 5/50\n",
      "106/106 [==============================] - 57s 534ms/step - loss: 0.2821 - accuracy: 0.8683 - val_loss: 0.3060 - val_accuracy: 0.8857\n",
      "Epoch 6/50\n",
      "106/106 [==============================] - 55s 519ms/step - loss: 0.2572 - accuracy: 0.8808 - val_loss: 0.2624 - val_accuracy: 0.8913\n",
      "Epoch 7/50\n",
      "106/106 [==============================] - 57s 536ms/step - loss: 0.2281 - accuracy: 0.9079 - val_loss: 0.2505 - val_accuracy: 0.9010\n",
      "Epoch 8/50\n",
      "106/106 [==============================] - 53s 501ms/step - loss: 0.2264 - accuracy: 0.9060 - val_loss: 0.3224 - val_accuracy: 0.8636\n",
      "Epoch 9/50\n",
      "106/106 [==============================] - 52s 492ms/step - loss: 0.2197 - accuracy: 0.9099 - val_loss: 0.2750 - val_accuracy: 0.8989\n",
      "Epoch 10/50\n",
      "106/106 [==============================] - 53s 496ms/step - loss: 0.1655 - accuracy: 0.9350 - val_loss: 0.2593 - val_accuracy: 0.8996\n",
      "Epoch 11/50\n",
      "106/106 [==============================] - 53s 500ms/step - loss: 0.1564 - accuracy: 0.9364 - val_loss: 0.2395 - val_accuracy: 0.9017\n",
      "Epoch 12/50\n",
      "106/106 [==============================] - 54s 508ms/step - loss: 0.1549 - accuracy: 0.9410 - val_loss: 0.2428 - val_accuracy: 0.9010\n",
      "Epoch 13/50\n",
      "106/106 [==============================] - 52s 495ms/step - loss: 0.1482 - accuracy: 0.9386 - val_loss: 0.3749 - val_accuracy: 0.8774\n",
      "Epoch 14/50\n",
      "106/106 [==============================] - 52s 493ms/step - loss: 0.1386 - accuracy: 0.9426 - val_loss: 0.3826 - val_accuracy: 0.8684\n",
      "Epoch 15/50\n",
      "106/106 [==============================] - 53s 505ms/step - loss: 0.1520 - accuracy: 0.9365 - val_loss: 0.2984 - val_accuracy: 0.8996\n",
      "Epoch 16/50\n",
      "106/106 [==============================] - 54s 509ms/step - loss: 0.1487 - accuracy: 0.9417 - val_loss: 0.2516 - val_accuracy: 0.9079\n",
      "Epoch 17/50\n",
      "106/106 [==============================] - 53s 498ms/step - loss: 0.1183 - accuracy: 0.9505 - val_loss: 0.2780 - val_accuracy: 0.8961\n",
      "Epoch 18/50\n",
      "106/106 [==============================] - 54s 508ms/step - loss: 0.1097 - accuracy: 0.9528 - val_loss: 0.3126 - val_accuracy: 0.9107\n",
      "Epoch 19/50\n",
      "106/106 [==============================] - 54s 511ms/step - loss: 0.0942 - accuracy: 0.9634 - val_loss: 0.3293 - val_accuracy: 0.8920\n",
      "Epoch 20/50\n",
      "106/106 [==============================] - 52s 491ms/step - loss: 0.1159 - accuracy: 0.9546 - val_loss: 0.3076 - val_accuracy: 0.9003\n",
      "Epoch 21/50\n",
      "106/106 [==============================] - 53s 496ms/step - loss: 0.0900 - accuracy: 0.9652 - val_loss: 0.4037 - val_accuracy: 0.8636\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 00021: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x291970d2370>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_new_train, y_new_train, batch_size = 32, epochs = 50, verbose = 1, \n",
    "          callbacks = [earlystopping, mcp_save, reduce_lr_loss], validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최종 모델 파라미터 가져오기\n",
    "앞서 CNN Modeling + Callbacks 과정을 통해 가장 validation loss를 최소화하는 모델 파라미터를 ModelCheckpoint를 이용해 저장을 했다. 이제 마지막으로 저장한 파라미터를 불러와서 그대로 evaluation을 진행하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 15s 101ms/step - loss: 0.1429 - accuracy: 0.9453\n",
      "Train score: 0.14286918938159943\n",
      "Train accuracy: 0.9453449845314026\n"
     ]
    }
   ],
   "source": [
    "# model evaluation\n",
    "model.load_weights(filepath = '.mdl_wts.hdf5')\n",
    "\n",
    "score = model.evaluate(X_new_train, y_new_train, verbose=1)\n",
    "print('Train score:', score[0])\n",
    "print('Train accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
